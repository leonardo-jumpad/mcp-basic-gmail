# ğŸš€ Parte 1 â€“ Tutorial bÃ¡sico **sem LangGraph**

Aqui vocÃª usa LangChain para chamar o modelo de linguagem direto, fazer perguntas e receber respostas simples.

### CÃ³digo bÃ¡sico:

```python
from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

pergunta = "O que Ã© LangChain?"

resposta = llm.invoke(pergunta)

print(resposta.content)
```

### O que estÃ¡ acontecendo?

-   VocÃª cria um objeto `llm` com o modelo da OpenAI.
-   Usa `llm.invoke()` para mandar o prompt.
-   Recebe a resposta com `resposta.content`.
-   Ã‰ uma chamada simples, direta, sem fluxo, memÃ³ria ou ferramentas.

---

### LangGraph e LangGraph?

-   **Sem LangGraph:** vocÃª chama o LLM direto, resposta simples e sem fluxo.
-   **Com LangGraph:** vocÃª monta um grafo com estados, mÃºltiplos nÃ³s e decisÃµes â€” para agentes mais inteligentes.
-   Pode adicionar ferramentas e lÃ³gica condicional

# ğŸš€ Parte 2 â€“ Exemplos com LangGraph

---

## Exemplo 1: Fluxo simples com um nÃ³ que responde

```python
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph
from typing import TypedDict

class Estado(TypedDict):
    mensagem: str

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

def responder(state: Estado) -> Estado:
    msg = state["mensagem"]
    resp = llm.invoke(f"Responda educadamente: {msg}")
    return {"mensagem": resp.content}

builder = StateGraph(Estado)
builder.add_node("responder", responder)
builder.set_entry_point("responder")
builder.set_finish_point("responder")

grafo = builder.compile()

entrada = {"mensagem": "O que Ã© LangGraph?"}
saida = grafo.invoke(entrada)

print(saida["mensagem"])
```

**ExplicaÃ§Ã£o:**
VocÃª cria um grafo com um nÃ³, ele recebe um estado com a mensagem e responde. FÃ¡cil para entender a estrutura do grafo.

---

## Exemplo 2: Fluxo condicional simples

```python
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph
from typing import TypedDict

class Estado(TypedDict):
    mensagem: str
    tipo: str

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

def classificar(state: Estado) -> Estado:
    msg = state["mensagem"].lower()
    if "oi" in msg or "olÃ¡" in msg:
        return {"mensagem": state["mensagem"], "tipo": "saudacao"}
    else:
        return {"mensagem": state["mensagem"], "tipo": "pergunta"}

def responder_saudacao(state: Estado) -> Estado:
    return {"mensagem": "OlÃ¡! Como posso ajudar?" , "tipo": "fim"}

def responder_pergunta(state: Estado) -> Estado:
    resp = llm.invoke(f"Responda: {state['mensagem']}")
    return {"mensagem": resp.content, "tipo": "fim"}

builder = StateGraph(Estado)
builder.add_node("classificar", classificar)
builder.add_node("saudacao", responder_saudacao)
builder.add_node("resposta", responder_pergunta)

builder.add_conditional_edges(
    "classificar",
    lambda s: s["tipo"],
    {"saudacao": "saudacao", "pergunta": "resposta"},
)

builder.set_entry_point("classificar")
builder.set_finish_point("saudacao")
builder.set_finish_point("resposta")

grafo = builder.compile()

print(grafo.invoke({"mensagem": "Oi, tudo bem?", "tipo": ""})["mensagem"])
print(grafo.invoke({"mensagem": "O que Ã© LangGraph?", "tipo": ""})["mensagem"])
```

**ExplicaÃ§Ã£o:**
Aqui o grafo decide para onde ir dependendo da mensagem: se for saudaÃ§Ã£o, responde com texto fixo; se for pergunta, chama o LLM.

---

## Exemplo 3: Fluxo com ferramenta simples (dobrar nÃºmero)

```python
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph
from langchain_core.tools import tool
from typing import TypedDict

class Estado(TypedDict):
    mensagem: str
    numero: float

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

@tool
def dobrar_numero(numero: float) -> str:
    return f"O dobro de {numero} Ã© {numero * 2}"

def responder(state: Estado) -> Estado:
    if "dobrar" in state["mensagem"]:
        resultado = dobrar_numero(state["numero"])
        return {"mensagem": resultado, "numero": state["numero"]}
    else:
        resp = llm.invoke(f"Responda: {state['mensagem']}")
        return {"mensagem": resp.content, "numero": state["numero"]}

builder = StateGraph(Estado)
builder.add_node("responder", responder)
builder.set_entry_point("responder")
builder.set_finish_point("responder")

grafo = builder.compile()

entrada = {"mensagem": "Por favor, dobre o nÃºmero", "numero": 5}
saida = grafo.invoke(entrada)

print(saida["mensagem"])
```

**ExplicaÃ§Ã£o:**
Aqui o agente usa uma ferramenta para dobrar nÃºmero, se a mensagem pedir. Caso contrÃ¡rio, usa o LLM.

---

---
